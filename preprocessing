# 설치
!pip install kss pykospacing symspellpy regex openai

# -*- coding: utf-8 -*-
"""
하이브리드 전처리 파이프라인 (무료 모델 중심 + 필요시 LLM 보정)
- 규칙: 정규화/하이픈/줄바꿈/헤더-푸터 제거
- 무료: kss 문장 분리, pykospacing 띄어쓰기(옵션), symspellpy 철자(옵션)
- 휴리스틱: 섹션 라벨(h2/h3/body/caption) 추정
- 게이팅: 저신뢰/이상치만 LLM 보정(Optional; OPENAI_API_KEY 필요)
- 출력: 청크 JSON + 메타데이터(section_path, neighbors, anchors)
"""

import os, json, argparse, unicodedata
from typing import Any, Dict, List, Optional
import regex as re

# --- 옵션 의존성 로딩 (없으면 자동 비활성) ---
try:
    import kss
    HAS_KSS = True
except ImportError:
    HAS_KSS = False

try:
    from pykospacing import Spacing
    SPACING = Spacing()
    HAS_SPACING = True
except Exception:
    HAS_SPACING = False
    SPACING = None

try:
    from symspellpy import SymSpell, Verbosity
    HAS_SYMSPELL = True
except Exception:
    HAS_SYMSPELL = False

# --- LLM (선택) ---
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", None)
try:
    from openai import OpenAI
    HAS_OPENAI = bool(OPENAI_API_KEY)
except Exception:
    HAS_OPENAI = False


# -------------------------
# 규칙 기반 유틸
# -------------------------
def normalize_nfc(t: str) -> str:
    t = unicodedata.normalize("NFKC", t)
    # 스마트 따옴표 통일
    t = t.replace("“","\"").replace("”","\"").replace("‘","'").replace("’","'")
    # 연속 공백/탭 정리
    t = re.sub(r"[ \t]{2,}", " ", t)
    return t.strip()

def unhyphenate_linebreak(t: str) -> str:
    # wor-\nking -> working (줄 끝 하이픈)
    return re.sub(r"(\w)-\n(\w)", r"\1\2", t)

def flatten_soft(t: str) -> str:
    # 문단 내부 줄바꿈은 공백으로(빈 줄 유지)
    t = re.sub(r"([^\n])\n(?!\n)", r"\1 ", t)
    t = re.sub(r"\n{3,}", "\n\n", t)
    return t

def collect_lines_by_page(ocr: Dict[str, Any]) -> Dict[int, List[str]]:
    out = {}
    for pg in ocr.get("pages", []):
        pno = pg["page"]
        lines = []
        for sp in pg.get("spans", []):
            txt = sp.get("text", "")
            if not txt: continue
            lines.append(normalize_nfc(txt))
        out[pno] = [l for l in lines if l]
    return out

def remove_repeated_edges(lines_by_page: Dict[int, List[str]], ratio=0.6) -> Dict[int, List[str]]:
    total = len(lines_by_page) or 1
    hdr_cnt, ftr_cnt = {}, {}
    for _, lines in lines_by_page.items():
        if not lines: continue
        hdr_cnt[lines[0]] = hdr_cnt.get(lines[0], 0) + 1
        ftr_cnt[lines[-1]] = ftr_cnt.get(lines[-1], 0) + 1
    hdr = {k for k,v in hdr_cnt.items() if v/total >= ratio}
    ftr = {k for k,v in ftr_cnt.items() if v/total >= ratio}

    cleaned = {}
    for p, lines in lines_by_page.items():
        out = []
        for i, ln in enumerate(lines):
            s = ln.strip()
            if (i==0 and s in hdr) or (i==len(lines)-1 and s in ftr):
                continue
            if re.fullmatch(r"\d{1,4}", s):  # 단독 페이지번호 제거
                continue
            out.append(ln)
        cleaned[p] = out
    return cleaned

# -------------------------
# 무료 모델 단계
# -------------------------
def sent_split(text: str) -> List[str]:
    if HAS_KSS:
        # 문단 단위로 kss 분할
        sents: List[str] = []
        for para in re.split(r"\n{2,}", text):
            para = para.strip()
            if not para: continue
            sents += kss.split_sentences(para)
        return [s.strip() for s in sents if s.strip()]
    # fallback: 마침표/물음표/느낌표/줄임표 기준
    return [s.strip() for s in re.split(r"(?<=[.!?…])\s+", text) if s.strip()]

def spacing_fix(text: str) -> str:
    if HAS_SPACING:
        try:
            return SPACING(text)
        except Exception:
            return text
    return text

def build_symspell(dict_path: Optional[str] = None) -> Optional[SymSpell]:
    if not HAS_SYMSPELL:
        return None
    try:
        sym = SymSpell(max_dictionary_edit_distance=2, prefix_length=7)
        # 기본 한국어 사전이 없다면, 사용자가 제공(옵션). 없으면 비활성.
        dict_path = dict_path or ""
        if dict_path and os.path.exists(dict_path):
            sym.load_dictionary(dict_path, term_index=0, count_index=1, separator=" ")
            return sym
    except Exception:
        return None
    return None

def symspell_fix(text: str, sym: Optional[Any]) -> str:
    if not sym:
        return text
    # 너무 공격적으로 교정하지 않도록 단어 길이/숫자/기호 필터
    tokens = re.findall(r"[가-힣A-Za-z]+|\d+|[^\s\w]", text)
    out = []
    for tok in tokens:
        if re.fullmatch(r"[가-힣A-Za-z]{3,}", tok):
            sug = sym.lookup(tok, Verbosity.CLOSEST, max_edit_distance=2)
            if sug and sug[0].term:
                out.append(sug[0].term)
            else:
                out.append(tok)
        else:
            out.append(tok)
    return "".join(out)

# -------------------------
# 라벨링 휴리스틱
# -------------------------
def label_line(s: str) -> str:
    t = s.strip()
    if not t:
        return "blank"
    # 번호 패턴으로 시작 + 마침표 없음 + 짧음 => 제목 가능
    if re.match(r"^(\d+(\.\d+)*|[IVXLCDM]+|[가-힣]\))\s", t) and not t.endswith('.'):
        return "h2"
    if len(t) <= 15 and not t.endswith('.'):
        return "h2"
    if re.match(r"^(표|그림)\s*\d+[:\.\)]", t):
        return "caption"
    if re.search(r"\d+\)$", t) and len(t) < 20:
        return "h3"
    return "body"

# -------------------------
# 게이팅(품질 점검)
# -------------------------
def anomaly_score(text: str) -> float:
    # 단순 이상치 점수: 너무 긴 토큰/기호 비율/깨진문자 비율
    if not text:
        return 0.0
    length = len(text)
    weird = len(re.findall(r"[^\w\s가-힣.,;:!?“”\"\'\-\(\)\[\]…]", text))
    long_tokens = sum(1 for tok in text.split() if len(tok) > 30)
    return min(1.0, (weird*0.01) + (long_tokens*0.1) + (0.0005*max(0,length-1200)))

def need_llm_boost(page_text: str, avg_conf: Optional[float], threshold_conf=0.85) -> bool:
    bad_conf = (avg_conf is not None) and (avg_conf < threshold_conf)
    bad_text = anomaly_score(page_text) > 0.2
    return bool(bad_conf or bad_text)

# -------------------------
# LLM 보정 (선택)
# -------------------------
def llm_refine_chunks(page_text: str, max_chars: int) -> List[Dict[str, Any]]:
    if not HAS_OPENAI:
        return []
    try:
        client = OpenAI(api_key=OPENAI_API_KEY)
        sys = (
            "너는 OCR 텍스트를 학습용 청크로 재구성하는 조력자다. "
            "출력은 반드시 JSON 배열. 설명 금지."
        )
        user = f"""
아래 텍스트를 {max_chars}자 이하 청크들로 나누고, 각 청크에 다음 키를 넣어 JSON 배열로 출력:
- section: "h2" | "h3" | "Body" 중 하나
- chunk_text: 청크 본문(자연스러운 문장 단위로)
- sentences: 핵심 문장 2~6개(원문에서 그대로)
규칙: 원문 순서 유지, 표/그림 캡션은 제외

텍스트:
\"\"\"{page_text}\"\"\"
"""
        resp = client.chat.completions.create(
            model="gpt-4o-mini",
            temperature=0.2,
            messages=[{"role":"system","content":sys},{"role":"user","content":user}],
            response_format={"type":"json_object"}
        )
        txt = resp.choices[0].message.content
        data = json.loads(txt)
        if isinstance(data, dict) and "chunks" in data:
            return data["chunks"]
        if isinstance(data, list):
            return data
    except Exception:
        pass
    return []

# -------------------------
# 청크화
# -------------------------
def chunk_by_length(sents: List[str], max_chars=900) -> List[str]:
    chunks, buf = [], ""
    for s in sents:
        if len(buf) + len(s) + 1 <= max_chars:
            buf = (buf + " " + s).strip()
        else:
            if buf: chunks.append(buf)
            buf = s
    if buf: chunks.append(buf)
    return chunks

# -------------------------
# 메인 파이프라인
# -------------------------
def run(ocr_json: Dict[str, Any],
        max_chars: int = 900,
        use_spacing: bool = True,
        use_symspell: bool = False,
        symspell_dict: Optional[str] = None,
        use_llm: bool = True) -> Dict[str, Any]:

    lines_map = collect_lines_by_page(ocr_json)
    lines_map = remove_repeated_edges(lines_map)

    sym = build_symspell(symspell_dict) if use_symspell else None

    all_chunks: List[Dict[str, Any]] = []
    prev_id = None
    idx = 0

    # OCR 평균 conf가 입력에 있을 수도, 없을 수도 있음: 페이지 단위로 추정
    conf_map: Dict[int, float] = {}
    for pg in ocr_json.get("pages", []):
        pno = pg["page"]
        confs = [float(sp.get("conf", 1.0)) for sp in pg.get("spans", []) if sp.get("conf") is not None]
        conf_map[pno] = sum(confs)/len(confs) if confs else None

    for p in sorted(lines_map.keys()):
        raw = "\n".join(lines_map[p])
        txt = unhyphenate_linebreak(raw)
        txt = flatten_soft(txt)

        # 무료 모델 보정
        if use_spacing and HAS_SPACING:
            txt = spacing_fix(txt)
        if use_symspell and sym:
            txt = symspell_fix(txt, sym)

        # LLM 게이팅
        refined = []
        if use_llm and need_llm_boost(txt, conf_map.get(p)):
            refined = llm_refine_chunks(txt, max_chars=max_chars)

        if refined:  # LLM이 성공적으로 구조화
            for ch in refined:
                idx += 1
                cid = f"c_{idx:05d}"
                item = {
                    "chunk_id": cid,
                    "page": p,
                    "section_path": [ch.get("section","Body")],
                    "text": ch.get("chunk_text",""),
                    "avg_conf": conf_map.get(p),
                    "flags": {"model_structured": True, "llm_refined": True},
                    "anchors": {"key_sents": ch.get("sentences", [])},
                    "neighbors": {"prev": prev_id, "next": None},
                    "source_span_ids": []
                }
                if prev_id:
                    all_chunks[-1]["neighbors"]["next"] = cid
                all_chunks.append(item)
                prev_id = cid
            continue

        # LLM을 쓰지 않거나 실패했으면: 휴리스틱 라벨 + 길이 청크
        sents = sent_split(txt)
        chunks = chunk_by_length(sents, max_chars=max_chars)

        # 페이지의 대표 라벨(간단): 첫문장 라벨 or 분포 기준
        def_section = "Body"
        if sents:
            lb = label_line(sents[0])
            def_section = "Body" if lb in ("body","blank","caption") else lb

        for c in chunks:
            # 핵심 문장(간단): 청크 앞쪽 2~5문장
            key_sents = sent_split(c)[:5] if len(c) < max_chars*1.2 else sents[:5]

            idx += 1
            cid = f"c_{idx:05d}"
            item = {
                "chunk_id": cid,
                "page": p,
                "section_path": [def_section],
                "text": c,
                "avg_conf": conf_map.get(p),
                "flags": {"model_structured": False, "llm_refined": False},
                "anchors": {"key_sents": key_sents},
                "neighbors": {"prev": prev_id, "next": None},
                "source_span_ids": []
            }
            if prev_id:
                all_chunks[-1]["neighbors"]["next"] = cid
            all_chunks.append(item)
            prev_id = cid

    return {"doc_id": ocr_json.get("doc_id", "doc"), "chunks": all_chunks}


# -------------------------
# CLI
# -------------------------
if __name__ == "__main__":
    ap = argparse.ArgumentParser()
    ap.add_argument("--input", required=True)
    ap.add_argument("--output", required=True)
    ap.add_argument("--max-chars", type=int, default=900)
    ap.add_argument("--use-spacing", type=str, default="true")
    ap.add_argument("--use-symspell", type=str, default="false")
    ap.add_argument("--symspell-dict", type=str, default="")
    ap.add_argument("--llm", type=str, default="true")
    args = ap.parse_args()

    use_spacing = args.use_spacing.lower() == "true"
    use_symspell = args.use_symspell.lower() == "true"
    use_llm = args.llm.lower() == "true"

    with open(args.input, "r", encoding="utf-8") as f:
        data = json.load(f)

    out = run(
        data,
        max_chars=args.max_chars,
        use_spacing=use_spacing,
        use_symspell=use_symspell,
        symspell_dict=args.symspell_dict or None,
        use_llm=use_llm
    )

    with open(args.output, "w", encoding="utf-8") as f:
        json.dump(out, f, ensure_ascii=False, indent=2)

    print(f"[OK] wrote {args.output}")


