!pip install -q openai pdfminer.six PyPDF2 python-dotenv

import os, re, json, math, textwrap
from typing import List, Dict, Any, Tuple

# --- 환경설정 ---
from dotenv import load_dotenv
load_dotenv()  # .env에서 OPENAI_API_KEY 로드
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
assert OPENAI_API_KEY, "환경변수 OPENAI_API_KEY가 필요합니다 (.env 또는 시스템 환경변수)."

from openai import OpenAI
client = OpenAI(api_key=OPENAI_API_KEY)  # 기본적으로 env를 읽지만, 명시적으로 넣어도 됨. :contentReference[oaicite:2]{index=2}

# .env 불러오기 -test code
load_dotenv()
api_key = os.getenv("OPENAI_API_KEY")

# 1. 키가 잘 불러와졌는지 확인
print("API KEY 불러오기:", api_key[:8] + "..." if api_key else "불러오기 실패")

# 2. 클라이언트 생성
client = OpenAI(api_key=api_key)

# 3. 아주 간단한 호출 (모델 목록 불러오기)
models = client.models.list()

print("사용 가능한 모델 수:", len(models.data))
print("첫번째 모델:", models.data[0].id)

# 사용할 모델 (텍스트 요약/개념추출 용)
MODEL = "gpt-4o-mini"   # 필요시 "gpt-4.1" 또는 경량 "gpt-4o-mini"로 조절 가능. :contentReference[oaicite:3]{index=3}
MAX_OUTPUT_TOKENS = 600       # 1600 → 600
TOP_K_CONCEPTS = 10           # 15 → 10 (프롬프트에 사용)

# --- PDF 경로 & 페이지 범위 ---
PDF_PATH = r"F:\Joseph\3-2\발달 심리학\week 4 토론\신경가소성_덕분에_우리는_배움을_멈추지_않는다_—_Wonderful_Mind(2).pdf"
PAGE_START = 1     # 1-indexed inclusive
PAGE_END   = 3  # None이면 끝까지

OUT_DIR = "./results"
os.makedirs(OUT_DIR, exist_ok=True)

# [ADDED] 429 대비 재시도 래퍼
import time
from openai import RateLimitError

def ask_openai_json_with_retry(prompt: str, model: str, max_output_tokens: int = 600,
                               retries: int = 3, backoff_sec: float = 5.0):
    for attempt in range(1, retries+1):
        try:
            return ask_openai_json(prompt, model=model, max_output_tokens=max_output_tokens)
        except RateLimitError as e:
            # 크레딧 부족 메시지는 즉시 안내
            if "insufficient_quota" in str(e):
                raise RuntimeError("OpenAI: insufficient_quota — 크레딧/결제 확인 필요.") from e
            if attempt == retries:
                raise
            sleep_for = backoff_sec * (2 ** (attempt-1))
            print(f"[429] Rate limit: {attempt}/{retries} 재시도 전 {sleep_for:.1f}s 대기…")
            time.sleep(sleep_for)


# --- PDF 텍스트 추출 (pdfminer 우선, 실패 시 PyPDF2) ---
def extract_text_by_page(path: str) -> List[str]:
    texts = []
    try:
        from pdfminer.high_level import extract_pages
        from pdfminer.layout import LTTextContainer
        for page_layout in extract_pages(path):
            page_text = []
            for element in page_layout:
                if isinstance(element, LTTextContainer):
                    page_text.append(element.get_text())
            texts.append("\n".join(page_text))
        if texts:
            return texts
    except Exception:
        pass
    try:
        import PyPDF2
        with open(path, "rb") as f:
            reader = PyPDF2.PdfReader(f)
            for i in range(len(reader.pages)):
                texts.append(reader.pages[i].extract_text() or "")
        return texts
    except Exception as e:
        raise RuntimeError(f"텍스트 추출 실패: {e}")

def normalize_spaces(s: str) -> str:
    return re.sub(r"\s+", " ", s).strip()

def get_page_range_text(pages: List[str], start: int=None, end: int=None) -> Tuple[str, List[int]]:
    n = len(pages)
    s = 1 if start is None else max(1, start)
    e = n if end is None else min(end, n)
    chosen = [normalize_spaces(p) for p in pages[s-1:e]]
    return "\n".join(chosen), list(range(s, e+1))

def build_prompt_for_concepts_and_summaries(text: str, pages: List[int], top_k: int = 15) -> str:
    """
    모델에 '텍스트 내 정보만 사용'을 강제하고, JSON으로만 응답 받도록 지시.
    - 핵심 개념: 용어 / 한줄 정의 / 중요도(1~5) / 근거 페이지
    - 요약: 300자 / 반페이지 / 한페이지 + 키워드(최대 12) + 근거 페이지
    """
    # 참고: Responses API는 자유 텍스트를 주로 반환하므로 JSON만 달라고 강하게 지시하고 파싱합니다. :contentReference[oaicite:4]{index=4}
    return f"""
역할: 너는 교재 요약·개념 추출 어시스턴트다.
규칙:
- 반드시 아래 JSON 스키마에 '정확히' 맞춰서만 출력하라. 설명 문구, 마크다운, 주석 금지. JSON 외 텍스트 금지.
- 제공한 텍스트 안의 사실만 사용하고 추측/외부지식 금지.
입력 메타:
- pages: {pages}

출력 JSON 스키마 예시:
{{
  "concepts": [
    {{"term":"...", "definition":"...", "importance":3, "pages":[12,13]}}
  ],
  "summaries": {{
    "short_300":"...", 
    "half_page":"...", 
    "full_page":"...", 
    "keywords":["...","..."], 
    "pages":[12,13,14]
  }}
}}

요구사항:
1) "concepts": 상위 {top_k}개 핵심 개념을 뽑아 용어(term), 한 줄 정의(definition), 중요도(1~5), 근거 페이지 배열(pages)을 채워라.
2) "summaries": 
   - short_300: 300자 내외의 핵심 요약
   - half_page: A4 반 페이지 내외의 요약
   - full_page: A4 한 페이지 내외의 요약
   - keywords: 핵심 키워드 최대 10개
   - pages: 요약의 근거가 된 페이지 목록
3) 모든 페이지 번호는 입력 pages 범위 내에서만 선택.
4) 반드시 JSON만 출력.

<<본문>>
{text}
"""

def ask_openai_json(prompt: str, model: str = MODEL, max_output_tokens: int = 1200) -> Dict[str, Any]:
    """
    OpenAI Responses API 호출 → output_text를 JSON으로 파싱.
    Python SDK 최신 예시처럼 responses.create 사용. :contentReference[oaicite:5]{index=5}
    """
    resp = client.responses.create(
        model=model,
        # instructions를 쓰거나, 아래처럼 input에 사용자 프롬프트를 담아도 됩니다. :contentReference[oaicite:6]{index=6}
        input=[{"role": "user", "content": [{"type":"input_text","text": prompt}]}],
        max_output_tokens=max_output_tokens,
        temperature=0.2,
    )
    text = resp.output_text  # SDK에서 텍스트 추출 헬퍼
    try:
        data = json.loads(text)
    except json.JSONDecodeError as e:
        # 만약 JSON이 미세하게 틀리면, 가장 단순한 복구: JSON 본문만 추출 시도
        # 실서비스에서는 "structured output" 옵션을 쓰거나 재시도 로직을 권장. (문서: Responses API) :contentReference[oaicite:7]{index=7}
        raise RuntimeError(f"JSON 파싱 실패: {e}\n--- raw ---\n{text[:1000]}")
    return data

# [ADDED] 단순 문자 길이 기준 청크 분할
def chunk_text(s: str, max_chars: int = 9000):
    chunks, start = [], 0
    while start < len(s):
        end = min(len(s), start + max_chars)
        chunks.append(s[start:end])
        start = end
    return chunks

# [ADDED] 부분 결과 병합용 프롬프트
def build_merge_prompt(partial_results: list, pages: list, top_k: int = 10):
    return f"""
역할: 부분 요약/개념 결과를 병합한다. 반드시 JSON만 출력.
규칙:
- 동일/유사 개념은 중복 제거. importance는 평균 또는 상향 반영(1~5 유지).
- summaries는 중복/상충 정리, keywords 최대 12개로 축약.
- pages는 입력 합집합 내에서만 선택.

입력:
{json.dumps(partial_results, ensure_ascii=False)}

출력 스키마:
{{
  "concepts":[{{"term":"...","definition":"...","importance":3,"pages":[...]}}],
  "summaries":{{"short_300":"...","half_page":"...","full_page":"...","keywords":["..."],"pages":[...]}}
}}
"""

# [ADDED] 청크별 호출 → 최종 병합
def ask_concepts_summaries_chunked(full_text: str, pages: list,
                                   per_chunk_chars: int = 9000,
                                   model: str = "gpt-4o-mini",
                                   max_output_tokens: int = 600,
                                   top_k: int = 10):
    chunks = chunk_text(full_text, max_chars=per_chunk_chars)
    partial = []
    for i, ch in enumerate(chunks, 1):
        p = build_prompt_for_concepts_and_summaries(ch, pages, top_k=top_k)
        print(f"[청크 {i}/{len(chunks)}] 호출…")
        out = ask_openai_json_with_retry(p, model=model, max_output_tokens=max_output_tokens)
        partial.append(out)
    if len(partial) == 1:
        return partial[0]
    merge_prompt = build_merge_prompt(partial, pages, top_k=top_k)
    merged = ask_openai_json_with_retry(merge_prompt, model=model, max_output_tokens=max_output_tokens)
    return merged


# 1) PDF 읽기
pages_text = extract_text_by_page(PDF_PATH)
total_pages = len(pages_text)
s = PAGE_START if PAGE_START else 1
e = total_pages if PAGE_END is None else min(PAGE_END, total_pages)

doc_text, used_pages = get_page_range_text(pages_text, s, e)
print(f"[INFO] 총 {total_pages}페이지 중, {s}–{e} 범위를 사용합니다.")

# 2) 프롬프트 구성
prompt = build_prompt_for_concepts_and_summaries(doc_text, used_pages, top_k=15)

# 3) OpenAI 호출
# [CHANGED] 경량/청크/재시도 사용
result = ask_concepts_summaries_chunked(
    full_text=doc_text,
    pages=used_pages,
    per_chunk_chars=9000,
    model=MODEL,
    max_output_tokens=MAX_OUTPUT_TOKENS,
    top_k=TOP_K_CONCEPTS
)


# 4) 저장
CONCEPTS_JSON = os.path.join(OUT_DIR, "concepts_llm.json")
SUMMARIES_JSON = os.path.join(OUT_DIR, "summaries_llm.json")

concepts = result.get("concepts", [])
summaries = result.get("summaries", {})

with open(CONCEPTS_JSON, "w", encoding="utf-8") as f:
    json.dump(concepts, f, ensure_ascii=False, indent=2)

with open(SUMMARIES_JSON, "w", encoding="utf-8") as f:
    json.dump(summaries, f, ensure_ascii=False, indent=2)

print("[OK] 저장 완료")
print(" - 핵심 개념:", CONCEPTS_JSON)
print(" - 3단 요약: ", SUMMARIES_JSON)

from pprint import pprint

print("=== 개념 상위 5개 미리보기 ===")
pprint(concepts[:5])
print("\n=== 300자 요약 ===")
print(textwrap.shorten(summaries.get("short_300",""), width=400, placeholder="…"))
print("\n=== 반페이지 요약(앞부분) ===")
print(textwrap.shorten(summaries.get("half_page",""), width=700, placeholder="…"))
print("\n=== 한페이지 요약(앞부분) ===")
print(textwrap.shorten(summaries.get("full_page",""), width=900, placeholder="…"))
print("\n키워드:", summaries.get("keywords", []))
print("근거 페이지:", summaries.get("pages", []))
